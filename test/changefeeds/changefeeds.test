#!/usr/bin/env python

raise NotImplementedError('This needs to be re-worked with access to shard boundries found through ReQL admin')

import collections, itertools, os, re, subprocess, sys, tempfile, threading, time, unittest, urllib2

lock = threading.Lock()

sys.path.append(os.path.join(os.path.dirname(__file__), os.path.pardir, 'common'))
import driver, utils

r = utils.import_python_driver()

def synchronized(lock):
	""" Synchronization decorator """
	def wrap(f):
		def newFunction(*args, **kw):
			with lock:
				return f(*args, **kw)
		return newFunction
	return wrap

def checkedCall(command):
	'''wrapper on subprocess.check_call to handle collecting stdout/stderr for errors'''
	
	output = tempfile.TemporaryFile('w+')
	try:
		subprocess.check_call(command, stdout=output, stderr=output)
	except subprocess.CalledProcessError as e:
		output.seek(0)
		raise subprocess.CalledProcessError(e.returncode, e.cmd, output=output.read())

class NextWithTimeout(threading.Thread):
	
	feed = None
	result = None
	daemon = True
	
	def __init__(self, feed):
		super(NextWithTimeout, self).__init__()
		self.feed = iter(feed)
	
	@classmethod
	def getNext(cls, feed, timeout=5):
		
		instance = NextWithTimeout(feed)
		instance.start()
		deadline = time.time() + timeout
		while time.time() < deadline:
			instance.join(.1)
			if not instance.isAlive():
				if isinstance(instance.result, Exception):
					raise instance.result
				return instance.result
		# we are leaving a daemon process run
		raise Exception('Timed out waiting %d seconds for next item' % timeout)
	
	def run(self):
		try:
			return next(self.feed)
		except Exception as e:
			self.result = e

class ChangefeedsTest_Base(unittest.TestCase):
	
	# -- settings
	
	dbName = 'test'
	tableName = 'changefeedIntegration'
	
	serversToStart = 4
	replicationLevel = 2
	recordsToGenerate = 1000
	samplesPerShard = 5
	splitStrings = ['Nc081680000000000#557'] # magic format
	
	# -- class variables
	
	servers = None
	shards = None # [{'range':[lower, upper], 'primary':MACHINE_NAME, 'secondary':[MACHINE_NAME, ...]}]
		
	# -- constants
	
	ServerEntry = collections.namedtuple('ServerEntry', ['process', 'connection', 'logfile', 'uuid'])
	ShardEntry = collections.namedtuple('ShardEntry', ['lower', 'lowerHTTP', 'lowerCLI', 'upper', 'upperHTTP', 'upperCLI', 'master', 'replicas', 'sample'])
	
	# --
	
	@classmethod
	def getShards(cls, refresh=False):
		
		if cls.shards is not None and refresh is False:
			return cls.shards
		
		server = cls.servers[0]
		adminConn = http_admin.ClusterAccess([(server.process.host, server.process.http_port)])
		
		cls.shards = []
		for lowerHTTP, upperHTTP in adminConn.find_table(cls.tableName).shard_ranges():
			if lowerHTTP in (None, ''):
				lower = None
				lowerHTTP = ''
				lowerCLI = ''
			elif lowerHTTP.startswith('N'):
				lower = int(lowerHTTP.split('%23')[1]) # ToDo: take the magic out of this, handle non-integer values
				lowerCLI = lowerHTTP.replace('%23', r'\23')
			elif lowerHTTP.startswith('S'):
				lower = lowerHTTP[1:]
				lowerCLI = lowerHTTP.replace('%23', r'\23')
			
			if upperHTTP in (None, ''):
				upper = None
				upperHTTP = ''
				upperCLI = ''
			elif upperHTTP.startswith('N'):
				upper = int(upperHTTP.split('%23')[1]) # ToDo: take the magic out of this, handle non-integer values
				upperCLI = upperHTTP.replace('%23', r'\23')
			elif upperHTTP.startswith('S'):
				upper = upperHTTP[1:]
				upperCLI = upperHTTP.replace('%23', r'\23')
			
			sample = [x['id'] for x in r.db(cls.dbName).table(cls.tableName).between(lower, upper).limit(1).run(server.connection)][0]
			
			cls.shards.append(cls.ShardEntry(lower=lower, lowerHTTP=lowerHTTP, lowerCLI=lowerHTTP.replace('%23', r'\23'), upper=upper, upperHTTP=upperHTTP, upperCLI=upperCLI, master=None, replicas=None, sample=sample))
		
		if len(cls.shards) == 0:
			sample = [x['id'] for x in r.db(cls.dbName).table(cls.tableName).between(None, None).limit(1).run(server.connection)][0]
			cls.shards.append(cls.ShardEntry(lower=None, lowerHTTP=None, lowerCLI=None, upper=None, upperHTTP=None, upperCLI=None, master=None, replicas=None, sample=sample))
		
		return cls.shards
	
	@classmethod
	def waitForShards(cls, timeout=20):
		
		connection = cls.servers[0].connection
		
		deadline = time.time() + 10
		lastError = None
		shards = cls.getShards()
		while time.time() < deadline:
			try:
				for shard in shards:
					r.db(cls.dbName).table(cls.tableName).get(shard.sample).run(connection)
				lastError = None
				break
			except r.RqlRuntimeError as e:
				lastError = e
				time.sleep(0.05)
		if lastError is not None:
			raise lastError
	
	@classmethod
	@synchronized(lock)
	def setUp(cls):
		
		# -- short-circut if we are alread setup
		
		if cls.servers is not None:
			return
		
		# -- start the servers
		
		cls.servers = []
		cluster = None
		for i in range (cls.serversToStart):
			logfile = tempfile.NamedTemporaryFile('w+')
			server = driver.Process(cluster=cluster, console_output=logfile.name)
			server.wait_until_started_up()
			cluster = server.cluster
			connection =  r.connect(host=server.host, port=server.driver_port)
			uuid = urllib2.urlopen('http://%s:%d/ajax/semilattice/me' % (server.host, server.http_port)).read().replace('"', '')
			cls.servers.append(cls.ServerEntry(process=server, connection=connection, logfile=logfile, uuid=uuid))
		
		primaryServer = cls.servers[0]
		
		# -- ensure db is available
		
		if cls.dbName not in r.db_list().run(primaryServer.connection):
			r.db_create(cls.dbName).run(primaryServer.connection)
		
		# -- setup test table
		
		if cls.tableName in r.db(cls.dbName).table_list().run(primaryServer.connection):
			r.db(cls.dbName).table_drop(cls.tableName).run(primaryServer.connection) # ensure we have a clean table
		r.db(cls.dbName).table_create(cls.tableName).run(primaryServer.connection)
		
		r.db(cls.dbName).table(cls.tableName).insert([{'id': x} for x in range(1, cls.recordsToGenerate + 1)]).run(primaryServer.connection)
		
		# -- shard and replicate the table and pin the two shards to the two servers
		
		# ToDo: convert this to ReQL commands when avalible
		
		adminCommandPrefix = [primaryServer.process.executable_path, 'admin', '--join', '%s:%d' % (primaryServer.process.host, primaryServer.process.cluster_port)]
		
		# - shard the table
		
		adminConn = http_admin.ClusterAccess([(primaryServer.process.host, primaryServer.process.http_port)])
		
		lastSplit = None
		for splitString in cls.splitStrings:
			adminConn.add_table_shard(cls.tableName, splitString)
		
		time.sleep(2)
		cls.waitForShards()
		
		# - parse out the shards created
		
		shards = cls.getShards(refresh=True)
		
		# - set replication level
		
		if cls.replicationLevel > 1:
			
			checkedCall(adminCommandPrefix + ['set', 'replicas', '%s.%s' % (cls.dbName, cls.tableName), str(cls.replicationLevel)])
			checkedCall(adminCommandPrefix + ['set', 'durability', '%s.%s' % (cls.dbName, cls.tableName), '--hard'])
		
		# - pin the shards
		
		if len(cls.servers) < cls.replicationLevel * len(shards):
			raise Exception('There are only %d servers, but we need %d (%d replicas x %d shards)' % (len(servers), cls.replicationLevel * len(cls.shards), cls.replicationLevel, len(cls.shards)))
		
		for i in range(len(shards)):
			shard = shards[i]
			
			# - master
			
			checkedCall(adminCommandPrefix + ['pin', 'shard', '%s.%s' % (cls.dbName, cls.tableName), '%s-%s' % (shard.lowerCLI, shard.upperCLI), '--master', cls.servers[i].uuid])
			cls.shards[i] = cls.shards[i]._replace(master=cls.servers[i])
			
			# - secondary
			
			if cls.replicationLevel > 1:
				uuids = []
				replicaServers = []
				for j in range(1, cls.replicationLevel):
					replicaServer = cls.servers[(cls.replicationLevel * j + 1) + i -1]
					replicaServers.append(replicaServer)
					uuids.append(replicaServer.uuid)
				checkedCall(adminCommandPrefix + ['pin', 'shard', '%s.%s' % (cls.dbName, cls.tableName), '%s-%s' % (shard.lowerCLI, shard.upperCLI), '--replicas', ','.join(uuids)])
				cls.shards[i] = cls.shards[i]._replace(replicas=replicaServers)
		
		# make sure everything is avalible
		
		time.sleep(10) # way too long, but newton is sometimes not showing the operations for quite some time
		cls.waitForShards()
	
	@classmethod
	def tearDown(cls):
		
		if cls.servers is None:
			return
		
		# verify that the servers are still running
		lastError = None
		for server in cls.servers:
			try:
				server.process.check()
			except Exception as e:
				lastError = e
		if lastError is not None:
			cls.servers = None
			raise e
	
	def makeChanges(self, samplesPerShard=None, connectionsToUse=None):
		'''make a number of minor changes to records, and return those ids'''
		
		if samplesPerShard is None:
			samplesPerShard = self.samplesPerShard
		
		if connectionsToUse is None:
			connectionsToUse = itertools.cycle([x.connection for x in self.servers])
		else:
			connectionsToUse = itertools.cycle(connectionsToUse)
		
		changedRecordIds = []
		shards = self.getShards()
		for lower, upper in [(x.lower, x.upper) for x in shards]:
			
			sampleIds = [x['id'] for x in r.db(self.dbName).table(self.tableName).between(lower, upper).sample(samplesPerShard).run(connectionsToUse.next())]
			
			for thisId in sampleIds:
				r.db(self.dbName).table(self.tableName).update({'id':thisId, 'changed':True}).run(connectionsToUse.next())
				changedRecordIds.append(thisId)
		
		changedRecordIds.sort()
		return changedRecordIds

class ChangefeedsTest(ChangefeedsTest_Base):
	'''Basic tests'''
	
	def test_simple(self):
		
		expectedCount = self.samplesPerShard * len(self.getShards())
		changefeed = r.db(self.dbName).table(self.tableName).changes().limit(expectedCount).run(self.servers[0].connection)
		
		expectedChangedIds = self.makeChanges()
		self.assertEqual(expectedChangedIds, sorted([x['new_val']['id'] for x in changefeed]))
	
	def test_multiple_servers(self):
		'''The same changefeed on multiple servers should get the same results'''
		
		expectedCount = self.samplesPerShard * len(self.getShards())
		changefeeds = [r.db(self.dbName).table(self.tableName).changes().limit(expectedCount).run(x.connection) for x in self.servers]
		
		# add data across all of the connections
		
		expectedResults = self.makeChanges()
		
		# verify that all of the feeds got the expected results
		
		for i in range(len(changefeeds)):
			feedResults = sorted([x['new_val']['id'] for x in changefeeds[i]])
			self.assertEqual(feedResults, expectedResults)

class ChangefeedsTest_Destructive(ChangefeedsTest_Base):
	'''Tests that mess with the servers'''
	
	@classmethod
	@synchronized(lock)
	def tearDown(cls):
		'''For destructive tests we close down everything after each test'''
		
		lastError = None
		for server in cls.servers:
			try:
				server.process.check_and_stop()
			except RuntimeError:
				pass
			except Exception as e:
				lastError = e
		cls.servers = None
		if lastError is not None:
			raise lastError
	
	def test_primary_falure(self):
		'''Test that we get the expected error when the master for a shard fails for a range'''
		
		server = self.servers[0]
		conn = server.connection
		
		# start the changefeed
		
		changefeed = r.db(self.dbName).table(self.tableName).changes().run(conn)
		
		# add a change and retrieve it
					
		r.db(self.dbName).table(self.tableName).insert({}).run(conn)
		NextWithTimeout.getNext(changefeed)
		
		# kill a primary server
		
		self.servers[1].process.kill()
		
		# check that we error
		
		self.assertRaises(r.RqlRuntimeError, NextWithTimeout.getNext, changefeed)

	def test_secondary_failure(self):
		'''Test when a secondary shardholder fails for a range'''
		
		conn = self.servers[0].connection
		
		# start the changefeed
		
		changefeed = r.db(self.dbName).table(self.tableName).changes().run(conn)
		
		# add a change and retrieve it
					
		r.db(self.dbName).table(self.tableName).insert({}).run(conn)
		NextWithTimeout.getNext(changefeed)
		
		# kill a secondary server
		
		self.servers[2].process.kill()
		
		# add another item and make sure we still work
		
		r.db(self.dbName).table(self.tableName).update({'id':1, 'updated':True}).run(conn)
		NextWithTimeout.getNext(changefeed)

	def test_connection_death(self):
		'''Test that the client handles the death of the server at the other end of the connection correctly'''
		
		server = self.servers[2]
		conn = server.connection
		conn2 = self.servers[0].connection
		
		# start the changefeed
		
		changefeed = r.db(self.dbName).table(self.tableName).changes().run(conn)
		
		# add a change and retrieve it
					
		r.db(self.dbName).table(self.tableName).insert({}).run(conn)
		NextWithTimeout.getNext(changefeed)
		
		# kill a secondary server
		
		self.servers[2].process.kill()
		
		# add another item and make sure we still work
		
		r.db(self.dbName).table(self.tableName).update({'id':1, 'updated':True}).run(conn2)
		
		# check that we error
		
		self.assertRaises(r.RqlDriverError, NextWithTimeout.getNext, changefeed)

# ===== main

def main():
    unittest.main()

if __name__ == '__main__':
    main()

// Copyright 2010-2015 RethinkDB, all rights reserved.
#ifndef CLUSTERING_TABLE_CONTRACT_EXECUTOR_HPP_
#define CLUSTERING_TABLE_CONTRACT_EXECUTOR_HPP_

#include "clustering/generic/raft_core.hpp"
#include "clustering/immediate_consistency/history.hpp"
#include "clustering/table_contract/contract_metadata.hpp"
#include "clustering/table_contract/cpu_sharding.hpp"
#include "clustering/table_contract/exec.hpp"

class store_subview_t;

/* The `contract_executor_t` is responsible for executing the instructions contained in
the `contract_t`s in the `table_raft_state_t`. Each server has one `contract_executor_t`
for each table it's a replica of. The `contract_executor_t` constantly monitors the
current Raft state and compares the contracts in the Raft state against its "executions",
which represent activities it is currently performing. As contracts appear and disappear
from the Raft state, it creates, updates, and destroys executions to match. It also takes
care of forwarding the `contract_ack_t`s generated by the executions back to the
`contract_coordinator_t`. */

class contract_executor_t {
public:
    contract_executor_t(
        const server_id_t &server_id,
        mailbox_manager_t *const mailbox_manager,
        raft_member_t<table_raft_state_t> *raft,
        watchable_map_t<std::pair<server_id_t, branch_id_t>, contract_execution_bcard_t>
            *remote_contract_execution_bcards,
        const multistore_ptr_t *multistore,
        branch_history_manager_t *branch_history_manager,
        const base_path_t &base_path,
        io_backender_t *io_backender,
        backfill_throttler_t *backfill_throttler,
        perfmon_collection_t *perfmons);

    watchable_map_t<std::pair<server_id_t, contract_id_t>, contract_ack_t> *get_acks() {
        return &ack_map;
    }

    watchable_map_t<std::pair<server_id_t, branch_id_t>, contract_execution_bcard_t>
            *get_local_contract_execution_bcards() {
        return &local_contract_execution_bcards;
    }

    watchable_map_t<uuid_u, table_query_bcard_t> *get_local_table_query_bcards() {
        return &local_table_query_bcards;
    }

private:
    /* The actual work of executing the contract--accepting queries from the user,
    performing backfills, etc.--is carried out by the three `execution_t` subclasses,
    `primary_execution_t`, `secondary_execution_t`, and `erase_execution_t`.
    `execution_data_t` is just a simple wrapper around an `execution_t` with some
    supporting objects. */
    class execution_data_t {
    public:
        /* The contract ID of the contract governing this execution. Note that this may
        change over the course of an execution; see the comment about `execution_key_t`.
        */
        contract_id_t contract_id;

        /* A `store_subview_t` containing only the sub-region of the store that this
        execution affects. */
        scoped_ptr_t<store_subview_t> store_subview;

        /* We create a new perfmon category for each execution. This way the executions
        themselves don't have to think about perfmon key collisions. */
        perfmon_collection_t perfmon_collection;
        scoped_ptr_t<perfmon_membership_t> perfmon_membership;

        /* The execution itself */
        scoped_ptr_t<execution_t> execution;
    };

    /* When a contract changes, we sometimes want to create a new execution and we
    sometimes want to update an existing one. Specifically, we want to create a new
    execution when:
    - The region of the contract changes
    - This server's role in the contract (primary, secondary, or neither) changes
    - This server's role is a secondary but the primary or branch has changed

    We implement this by computing an `execution_key_t` based on each contract, using the
    `get_contract_key()` function. If the old and new contracts have the same
    `execution_key_t`, then we update the corresponding execution. But if they differ,
    then we delete the old execution and create a new one. */
    class execution_key_t {
    public:
        enum class role_t { primary, secondary, erase };
        /* This is for generating perfmon keys */
        std::string role_name() const {
            switch (role) {
                case role_t::primary: return "primary";
                case role_t::secondary: return "secondary";
                case role_t::erase: return "erase";
                default: unreachable();
            }
        }
        /* This is just so we can use it as a `std::set`/`std::map` key */
        bool operator<(const execution_key_t &k) const {
            return std::tie(region, role, primary, branch) <
                std::tie(k.region, k.role, k.primary, k.branch);
        }
        region_t region;
        role_t role;
        server_id_t primary;
        branch_id_t branch;
    };

    execution_key_t get_contract_key(const std::pair<region_t, contract_t> &pair);

    /* `on_raft_state_change()` is called whenever the Raft state changes. In response to
    the Raft state change, we want to delete existing executions and spawn new ones.
    However, deleting executions may block. So `on_raft_state_change()` spawns
    `update_coro()`. `update_coro()` calls `apply_read()` on the Raft state watchable and
    passes the result to `update()`. `update()` may spawn new executions, but it may not
    delete them, because that would block. Instead, it puts their regions in
    `to_delete_out`, and then `update_coro()` deletes them. */
    void on_raft_state_change();
    void update_coro(auto_drainer_t::lock_t);
    void update(const table_raft_state_t &new_state,
                std::set<execution_key_t> *to_delete_out);

    /* This will send `cid` and `ack` to the coordinator. We pass it as a callback to
    the `execution_t`. */
    void send_ack(const execution_key_t &key, const contract_id_t &cid,
        const contract_ack_t &ack);

    const server_id_t server_id;
    raft_member_t<table_raft_state_t> *const raft;
    const multistore_ptr_t *const multistore;
    perfmon_collection_t *const perfmons;

    /* `ack_map` contains the `contract_ack_t`s created by our execution of contracts.
    It will be sent over the network to the `contract_coordinator_t` via the minidir. */
    watchable_map_var_t<std::pair<server_id_t, contract_id_t>, contract_ack_t> ack_map;

    /* `local_contract_execution_bcards` contains the `contract_execution_bcards` for
    our `primary_execution_t`s. It will be sent over the network to the other
    `contract_executor_t`s for this table, via the minidir, so that they can request
    backfills from us and connect their `listener_t`s to our `broadcaster_t`s. */
    watchable_map_var_t<std::pair<server_id_t, branch_id_t>, contract_execution_bcard_t>
        local_contract_execution_bcards;

    /* `local_table_query_bcards` contains the `table_query_bcard_t`s for our
    `primary_execution_t`s. It will be sent over the network to all the servers in the
    cluster, via the directory, so that they can run queries. */
    watchable_map_var_t<uuid_u, table_query_bcard_t> local_table_query_bcards;

    /* This is just a convenient struct to hold a bunch of objects that the
    `execution_t`s need access to. */
    execution_t::context_t execution_context;

    std::map<execution_key_t, scoped_ptr_t<execution_data_t> > executions;

    /* True if `update_coro()` is currently running. We use this to avoid spawning
    multiple redundant copies. */
    bool update_coro_running;

    /* Used to generate unique names for perfmons */
    int perfmon_counter;

    /* `drainer` makes sure that `update_coro()` stops before the other member variables
    are deleted. */
    auto_drainer_t drainer;

    /* We subscribe to changes in the Raft committed state so we can find out when a new
    contract has been issued. */
    watchable_t<raft_member_t<table_raft_state_t>::state_and_config_t>::subscription_t
        raft_state_subs;
};

#endif /* CLUSTERING_TABLE_CONTRACT_EXECUTOR_HPP_ */

